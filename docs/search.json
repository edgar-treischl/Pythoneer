[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pythoneer",
    "section": "",
    "text": "Preface\nThe Pythoneer book is work in progress and at a very early stage. The goal is to provide a guide for R users to transition into Python. The project is open-source and contributions are welcome. There are several reasons why it can be beneficial for R users to learn Python:\n\nVersatility: While R is a great language for statistical analysis and data visualization, Python is a general-purpose language that can be used for a wider variety of tasks, such as web development, machine learning, and scientific computing.\nLarge and active community: Python has a larger and active community, which means that there are a lot of resources and support available if you run into problems or want to learn new things.\nPopularity in industry: Python is widely used in industry, particularly in areas such as data science and machine learning, so learning Python can make you more competitive in the job market.\nIntegration with other tools: Python has many libraries and packages that allow it to integrate with other tools, such as databases, big data frameworks, and cloud computing platforms, making it a flexible and powerful tool for data analysis and computation.\nDifferent approaches to problem-solving: R and Python have different approaches to problem-solving and data analysis, so learning both can expand your skill set and enable you to tackle a wider range of problems.\n\n\n#Print with Python\nprint(\"Hello Python World\")\n\nHello Python World"
  },
  {
    "objectID": "intro.html#how-not-to-calculate-basic-statistics",
    "href": "intro.html#how-not-to-calculate-basic-statistics",
    "title": "1  Introduction",
    "section": "How (not) to calculate basic statistics",
    "text": "How (not) to calculate basic statistics\nBefore we can do anything, we have to install Python, learn how to manage virtual environments, and we how we can install packages. As with R, you have to install and import libraries for most tasks. We will cover these steps in the next chapter. For now, we will focus on some code snippets only. Say we use the seaborn package because it provides us with some data. We can use the mpg data set as cars and fool around with it. Only if you have never seen the mpg data set, you can use the following code snippet to get an overview of the data set.\n\n#import seaborn to get some data\nimport seaborn as sns\n\n#use the mpg data set as cars\ncars = sns.load_dataset(\"mpg\") \nprint(cars)\n\n#&gt;       mpg  cylinders  ...  origin                       name\n#&gt; 0    18.0          8  ...     usa  chevrolet chevelle malibu\n#&gt; 1    15.0          8  ...     usa          buick skylark 320\n#&gt; 2    18.0          8  ...     usa         plymouth satellite\n#&gt; 3    16.0          8  ...     usa              amc rebel sst\n#&gt; 4    17.0          8  ...     usa                ford torino\n#&gt; ..    ...        ...  ...     ...                        ...\n#&gt; 393  27.0          4  ...     usa            ford mustang gl\n#&gt; 394  44.0          4  ...  europe                  vw pickup\n#&gt; 395  32.0          4  ...     usa              dodge rampage\n#&gt; 396  28.0          4  ...     usa                ford ranger\n#&gt; 397  31.0          4  ...     usa                 chevy s-10\n#&gt; \n#&gt; [398 rows x 9 columns]\n\n\nSuppose a mean function is not implemented in Python. We can create our own function to calculate the mean. We need to define a function that calculates the mean of an array. First, we have to define the function with the def keyword, followed by the name of the function and the input value. The function should return the sum of the array divided by the length of the array.\n\n# Define \ndef mean (array): \n    n = len(array)\n    return sum(array) / n\n\n#A\nmean(cars.mpg)\n\n#&gt; 23.514572864321615\n\n\nThat’s a cool way to show us how a function works. We have to provide a name of a function and tell Python what the function does. I always learned how functions and other concepts work in much more artificial way, especially in classes courses about Python. Look how I learned how a function works in Python:\n\n#Create a function that inserts the name of the input value into a sentence\n\ndef hello(name):\n    return (f\"Hallo, {name}! How are you?\")\n\n# Call the function and give it value as input\nhello(\"Edgar\")\n\n#&gt; 'Hallo, Edgar! How are you?'\n\n\nThus, we create the hello function that returns a sentence and inserts the name of the input value. Nothing wrong about that, even calculating a mean seems a little bit more realistic to illustrate why we need such a function. Anyway, in some classes I learned how to calculate the mean, the median, and the modus. Maybe a function to calculate the variance or others measure of central tendency. You know what Pearson R is? Guess what the next code does? And I guess you will skip the code after line 2, well that’s what recommend since the code only illustrates my point. The next console shows how to calculate the correlation coefficient between two arrays and I actually copied to code from a book.\n\nimport math\ndef correlation(x, y):\n    n = len(x) \n    \n    # Means\n    x_mn = sum(x) / n \n    y_mn = sum(y) / n\n    \n    # Variance\n    var_x = (1 / (n-1)) * sum(map(lambda xi: (xi - x_mn) ** 2 , x)) \n    var_y = (1 / (n-1)) * sum(map(lambda yi: (yi - y_mn) ** 2 , y))\n    \n    # Std\n    std_x, std_y = math.sqrt(var_x), math.sqrt(var_y)\n    \n    # Covariance\n    xy_var = map(lambda xi, yi: (xi - x_mn) * (yi - y_mn), x, y) \n    cov = (1 / (n-1)) * sum(xy_var)\n    \n    # Pearson's R\n    r = cov / (std_x * std_y) \n    return float(f\"{r:.3f}\")\n\n# Some data\nsize = [20, 15, 40, 25, 35]\ncost = [300, 400, 600, 700, 666]\n\nprint(correlation(size, cost))\n\n#&gt; 0.666\n\n\nPlease, use numpy to get the scientific toolkit and pandas for tabular processing and the presentation of data. The word numpy stands for numerical Python and is a package that provides us with a lot of functions to work with arrays. The pandas package is a data manipulation and analysis library that provides us with data structures and functions to manipulate data. We can use the numpy package to calculate the correlation coefficient between two arrays. The numpy package provides us with the corrcoef function that takes two arrays as input and returns the correlation coefficient.\n\n# corrcoef function from numpy (short: np): \nimport numpy as np\nnp.corrcoef(size, cost)\n\n#&gt; array([[1.        , 0.66645893],\n#&gt;        [0.66645893, 1.        ]])\n\n\nThe pandas package provides us with the describe function that returns an overview of the data. We can use the mean and std functions to calculate the mean and standard deviation of the data.\n\nimport pandas as pd\n\ncars.describe()\n\n#&gt;               mpg   cylinders  ...  acceleration  model_year\n#&gt; count  398.000000  398.000000  ...    398.000000  398.000000\n#&gt; mean    23.514573    5.454774  ...     15.568090   76.010050\n#&gt; std      7.815984    1.701004  ...      2.757689    3.697627\n#&gt; min      9.000000    3.000000  ...      8.000000   70.000000\n#&gt; 25%     17.500000    4.000000  ...     13.825000   73.000000\n#&gt; 50%     23.000000    4.000000  ...     15.500000   76.000000\n#&gt; 75%     29.000000    8.000000  ...     17.175000   79.000000\n#&gt; max     46.600000    8.000000  ...     24.800000   82.000000\n#&gt; \n#&gt; [8 rows x 7 columns]\n\n\nWe have to append the describe() function to the saved cars data in order to get an overview of the central tendency measures. You can do the same with all other functions that calculate other measures, such as the mean or standard deviance:\n\ncars.mean()\n\n#&gt; mpg               23.514573\n#&gt; cylinders          5.454774\n#&gt; displacement     193.425879\n#&gt; horsepower       104.469388\n#&gt; weight          2970.424623\n#&gt; acceleration      15.568090\n#&gt; model_year        76.010050\n#&gt; dtype: float64\n\ncars.std()\n\n#&gt; mpg               7.815984\n#&gt; cylinders         1.701004\n#&gt; displacement    104.269838\n#&gt; horsepower       38.491160\n#&gt; weight          846.841774\n#&gt; acceleration      2.757689\n#&gt; model_year        3.697627\n#&gt; dtype: float64\n\n\nAnd we should at least look at the scatter plot since we talk about correlations. The matplotlib provides a lot of different graphs for us. And in case you are an R user, you can even use the ggplot2 style, just to let you show some possibilities. But of course, we have to think more systematically how we can reach our goal in the next chapter.\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\n\nplt.scatter(cars.mpg, cars.horsepower)"
  },
  {
    "objectID": "intro.html#summary",
    "href": "intro.html#summary",
    "title": "1  Introduction",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nThis chapter provided you with a brief overview of the book. We discussed the motivation behind this book, the target audience, and the structure of the book. We also outlined how you can transfer your R knowledge to Python. In the next chapter, we will discuss how to install Python, manage virtual environments, and install packages."
  },
  {
    "objectID": "firststeps.html",
    "href": "firststeps.html",
    "title": "2  First steps",
    "section": "",
    "text": "Python and R are both popular programming languages used for data analysis and machine learning tasks. While both languages have their own strengths and weaknesses, there are situations where using both languages together can be beneficial.\nusing Python and R together can be beneficial when you want to take advantage of the strengths of both languages. Whether you want to use R libraries in Python, Python libraries in R, or combine both languages in a single application, there are many options available for integrating Python and R. Here are some ways in which Python can be used with R:\n\nR can be called from Python using the “rpy2” package: rpy2 is a Python package that allows Python programs to call R functions and use R data structures. This can be useful when you want to use an R library that is not available in Python or when you have existing R code that you want to integrate into your Python program.\nJupyter Notebooks can be used to combine Python and R code: Jupyter Notebooks are interactive documents that allow you to combine code, text, and visualizations in a single document. Jupyter Notebooks support both Python and R, so you can use both languages in the same notebook.\nShiny applications can be built using Python: Shiny is a popular web framework for building interactive web applications in R. However, Shiny also supports using Python through the “reticulate” package. This can be useful when you want to use a Python library that is not available in R or when you have existing Python code that you want to integrate into your Shiny application.\nFinally, Python can be called from R using the reticulate package: Reticulate is a package that allows R programs to call Python functions and use Python data structures. This can be useful when you want to use a Python library that is not available in R or when you have existing Python code that you want to integrate into your R program (Ushey, Allaire, and Tang 2023).\n\nThe reticulate package makes it easy for R users to incorporate Python functionality into their R workflows, whether it’s using Python libraries not available in R, reusing existing Python code, or simply taking advantage of Python’s strengths in specific areas such as machine learning or web development.\n\nPython environment management: The package provides tools to manage Python environments within R, including creating and configuring virtual environments and managing package dependencies.\nCalling Python code: The package provides functions to call Python code from R, including importing Python modules, calling Python functions, and accessing Python objects. Passing data between R and Python: The package allows for seamless integration between R and Python data structures. For example, R data frames can be converted to Python pandas data frames and vice versa.\nInteractive sessions: The package provides an interactive Python console within R sessions, allowing users to execute Python commands interactively.\nPlotting: The package allows R users to create Python plots using popular Python visualization libraries such as Matplotlib and Seaborn.\n\nFirst, you need to have Python installed on your system. You can download Python from the official website (https://www.python.org/downloads/) and install it following the instructions provided. Next, install the reticulate package: Open an R session and install the reticulate package by running the following command:\n\ninstall.packages(\"reticulate\")\n\nOnce the package is installed, you need to set the Python path in R. The Python path is the location of the Python executable on your system. You can set the path by running the following command:\n\nlibrary(reticulate)\nuse_python(\"/Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/bin/python\")\n\nIf you’re using the default Python installation on your system, you can skip this step. You can test the installation by running the following command:\n\npy_config()\n\npython:         /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/bin/python\nlibpython:      /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/lib/libpython3.8.dylib\npythonhome:     /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate:/Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate\nversion:        3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:13)  [Clang 14.0.6 ]\nnumpy:          /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/lib/python3.8/site-packages/numpy\nnumpy_version:  1.24.2\n\nNOTE: Python version was forced by use_python() function\n\n\nThis command should display information about your Python installation, including the Python version, the location of the Python executable, and the Python library path. Once the reticulate package is installed and configured, you can start using Python functionality in your R code. For example, you can import Python modules, call Python functions, and access Python objects directly from R.\nHowever, I’ll use a virtual environment for the book and the conda_list() functions returns all (conda) environment of the system.\n\n#List all conda installations\nconda_list()\n\n            name\n1           base\n2       EdgarGPT\n3      IliartBot\n4 IliartGPT.venv\n5          flask\n6         fuckit\n7   r-reticulate\n8 streamlit.venv\n                                                                 python\n1                     /Users/edgar/Library/r-miniconda-arm64/bin/python\n2       /Users/edgar/Library/r-miniconda-arm64/envs/EdgarGPT/bin/python\n3      /Users/edgar/Library/r-miniconda-arm64/envs/IliartBot/bin/python\n4 /Users/edgar/Library/r-miniconda-arm64/envs/IliartGPT.venv/bin/python\n5          /Users/edgar/Library/r-miniconda-arm64/envs/flask/bin/python\n6         /Users/edgar/Library/r-miniconda-arm64/envs/fuckit/bin/python\n7   /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/bin/python\n8 /Users/edgar/Library/r-miniconda-arm64/envs/streamlit.venv/bin/python\n\n\nVirtual environments in Python are a useful tool for managing dependencies and ensuring that your project runs smoothly on different systems. Here are several reasons why it is wise to use virtual environments in Python:\n\nIsolation: When you create a virtual environment, you create a self-contained Python environment with its own installation of Python and any required packages. This means that the packages installed in one virtual environment do not interfere with packages installed in other virtual environments or the global Python environment. Isolating packages in this way reduces the risk of package version conflicts and makes it easier to manage dependencies.\nReproducibility: By using virtual environments, you can ensure that your code runs in a consistent and reproducible environment, regardless of the system it is run on. This is especially important if you plan to share your code with others or if you need to run your code on multiple systems.\nFlexibility: Virtual environments make it easy to switch between different Python versions or package configurations. This can be useful if you need to work on multiple projects that require different versions of Python or different package dependencies.\nSecurity: When you install packages in a virtual environment, you are not affecting the global Python installation on your system. This means that any security vulnerabilities or issues with the packages you install are contained within the virtual environment, reducing the risk of affecting other parts of your system.\n\nCreate a Python environment using the virtualenv_create() function. This function creates a new virtual environment and installs the specified packages. For example, to create a virtual environment called “myenv” and install the pandas package, you can run the following command:\n\nvirtualenv_create(\"myenv\", packages = \"pandas\")\n\nFinally, you can install additional Python packages using the py_install() function. This function installs the specified packages in the active Python environment. For example, to install the pandas package, you can run the following command:\n\npy_install(\"pandas\")\n\nThe next console achieves the same result with Conda and import the Pandas module.\n\n#install packages\nconda_install(\"r-reticulate\", \"pandas\")\npandas &lt;- import(\"pandas\")\n\nLet us explore some Python packages to see how we can intregrate them in R. For example, Seaborn is a visualization library based on Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics. It even gives us access to know data sets such as mtcars or Anscombe’s quartet.\nAnscombe’s quartet is a set of four datasets that have nearly identical descriptive statistics, yet have very different plots when graphed. The quartet was created by the statistician Francis Anscombe in 1973 to demonstrate the importance of visualizing data and the limitations of summary statistics. Each dataset consists of 11 (x, y) pairs, and the four datasets are designed to have the same mean, variance, correlation, and linear regression line. However, when plotted, each dataset reveals a very different relationship between x and y. The quartet is often used to illustrate the importance of data visualization and exploratory data analysis in understanding and interpreting statistical results.\n\nimport seaborn as sns\nsns.set_theme(style=\"ticks\")\n\n# Load the example dataset for Anscombe's quartet\ndf = sns.load_dataset(\"anscombe\")\n\n# Show the results of a linear regression within each dataset\nsns.lmplot(\n    data=df, x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\",\n    col_wrap=2, palette=\"muted\", ci=None,\n    height=4, scatter_kws={\"s\": 50, \"alpha\": 1}\n)\n\n\n\n\n\n\n\nLet us load the “mpg” dataset and display the first few rows of the data. If you haven’t already, you’ll need to install Seaborn first (Waskom 2021).\nLoad Seaborn and the “mpg” dataset: Once you have Seaborn installed, you can load it and the “mpg” dataset using the following Python code. It imports the Seaborn package as “sns” and loads the “mpg” dataset into a variable called “cars”. Moreover, I also import the Pandas package which we will use as well.\n\n#import numpy as np\n#import pandas as pd\nimport seaborn as sns\nimport pandas as pd\n\ncars = sns.load_dataset(\"mpg\")\n\nTo display the first few rows of the “mpg” dataset, you can use the head() function from the Pandas library, which is included in the Seaborn package:\n\n#print(cars)\ncars.head()\n\n    mpg  cylinders  displacement  ...  model_year  origin                       name\n0  18.0          8         307.0  ...          70     usa  chevrolet chevelle malibu\n1  15.0          8         350.0  ...          70     usa          buick skylark 320\n2  18.0          8         318.0  ...          70     usa         plymouth satellite\n3  16.0          8         304.0  ...          70     usa              amc rebel sst\n4  17.0          8         302.0  ...          70     usa                ford torino\n\n[5 rows x 9 columns]\n\n\nThe Pandas library is a popular data analysis toolkit for Python. It provides a wide range of functions and tools for working with structured data. For example, the describe() function is used to generate descriptive statistics of a Pandas DataFrame or Series. When called on a DataFrame or Series, the describe() function provides summary statistics such as count, mean, standard deviation, minimum, maximum, and quartiles. The next console shows how to generate descriptive statistics for the “mpg” dataset using the describe() function, you can use the following Python code:\n\ncars.describe()\n\n              mpg   cylinders  ...  acceleration  model_year\ncount  398.000000  398.000000  ...    398.000000  398.000000\nmean    23.514573    5.454774  ...     15.568090   76.010050\nstd      7.815984    1.701004  ...      2.757689    3.697627\nmin      9.000000    3.000000  ...      8.000000   70.000000\n25%     17.500000    4.000000  ...     13.825000   73.000000\n50%     23.000000    4.000000  ...     15.500000   76.000000\n75%     29.000000    8.000000  ...     17.175000   79.000000\nmax     46.600000    8.000000  ...     24.800000   82.000000\n\n[8 rows x 7 columns]\n\n\nOr consider the mean() function which is used to calculate the arithmetic mean of a Pandas DataFrame or Series. When called on a DataFrame or Series, the mean() function calculates the average value of all the elements in the DataFrame or Series.\n\ncars.mean()\n\nmpg               23.514573\ncylinders          5.454774\ndisplacement     193.425879\nhorsepower       104.469388\nweight          2970.424623\nacceleration      15.568090\nmodel_year        76.010050\ndtype: float64\n\n&lt;string&gt;:1: FutureWarning: The default value of numeric_only in DataFrame.mean is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n\n\nScipy is a popular library for scientific computing in Python, and it provides functions for calculating statistical values, such as the correlation coefficient. Start by importing the necessary packages. You’ll need the scipy.stats module to calculate the correlation coefficient, and you may also need numpy to work with arrays or matrices of data.\n\nimport scipy\nimport scipy.stats as stats\nimport numpy as np\n\nThe scipy.stats module can calculate the correlation coefficient for two arrays, but if you have more than two variables, you’ll need to use a matrix. Use the stats.pearsonr() function to calculate the Pearson correlation coefficient. This function takes two arguments: the two arrays or matrices to compare, and it returns two values: the correlation coefficient and the p-value.\n\nmpg = cars[\"mpg\"]\nhorsepower = cars[\"horsepower\"]\n\nscipy.stats.pearsonr(mpg, horsepower)\n\narray must not contain infs or NaNs\n\n\nUnfortuntely, there is a missing values problem that we need to fix first. In Python, missing values are typically represented by the special value NaN (Not a Number), which is part of the numpy library. There are different ways to drop missing values from a dataset, depending on the context and the desired outcome.\nDrop rows or columns with missing values: If you have missing values in your dataset and you want to remove entire rows or columns that contain at least one missing value, you can use the dropna() method. This method removes all rows that contain at least one missing value by default, but you can specify the argument axis=1 to remove columns instead.\nIf you want to drop missing values only within a specific column, you can use the dropna() method with the subset argument. This argument specifies the column or columns to consider when dropping missing values. The next console shows the first approaches and the results of the correlation coeficient.\n\ncars = cars.dropna()\nmpg = cars[\"mpg\"]\nhorsepower = cars[\"horsepower\"]\n\nscipy.stats.pearsonr(mpg, horsepower)\n\nPearsonRResult(statistic=-0.7784267838977761, pvalue=7.031989029403436e-81)\n\n\nAlternatively, if you have a pandas DataFrame, you can use the pandas.DataFrame.corr() method to calculate the correlation coefficient for all pairs of columns.\n\ncorr_matrix = cars.corr(method='pearson')\n\n&lt;string&gt;:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n\nprint(corr_matrix)\n\n                   mpg  cylinders  ...  acceleration  model_year\nmpg           1.000000  -0.777618  ...      0.423329    0.580541\ncylinders    -0.777618   1.000000  ...     -0.504683   -0.345647\ndisplacement -0.805127   0.950823  ...     -0.543800   -0.369855\nhorsepower   -0.778427   0.842983  ...     -0.689196   -0.416361\nweight       -0.832244   0.897527  ...     -0.416839   -0.309120\nacceleration  0.423329  -0.504683  ...      1.000000    0.290316\nmodel_year    0.580541  -0.345647  ...      0.290316    1.000000\n\n[7 rows x 7 columns]\n\n\nIn the next two chapter we learn more about data preparation, analysis, and visualization with Python. For example, the statsmodels.formula.api module is used to create and fit a linear regression model. The formula for the regression model is defined using a string that specifies the dependent variable and the independent variables, separated by ~ operator which is very similar compared to R. The ols() method is used to create the model, passing in the formula and the dataset. Finally, the fit() method is used to fit the model to the data, and the summary() method is used to print a summary of the model results.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nresults = smf.ols('mpg ~ horsepower', data=cars).fit()\nprint(results.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                    mpg   R-squared:                       0.606\nModel:                            OLS   Adj. R-squared:                  0.605\nMethod:                 Least Squares   F-statistic:                     599.7\nDate:                Sat, 01 Jun 2024   Prob (F-statistic):           7.03e-81\nTime:                        20:56:06   Log-Likelihood:                -1178.7\nNo. Observations:                 392   AIC:                             2361.\nDf Residuals:                     390   BIC:                             2369.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     39.9359      0.717     55.660      0.000      38.525      41.347\nhorsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n==============================================================================\nOmnibus:                       16.432   Durbin-Watson:                   0.920\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\nSkew:                           0.492   Prob(JB):                     0.000175\nKurtosis:                       3.299   Cond. No.                         322.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2023. “Reticulate: Interface to ’Python’.” https://CRAN.R-project.org/package=reticulate.\n\n\nWaskom, Michael L. 2021. “Seaborn: Statistical Data Visualization.” Journal of Open Source Software 6 (60): 3021. https://doi.org/10.21105/joss.03021."
  },
  {
    "objectID": "manipulation.html#filter",
    "href": "manipulation.html#filter",
    "title": "3  Data manipulation",
    "section": "3.1 Filter",
    "text": "3.1 Filter\nA common task in data manipulation is to filter the dataset. Suppose you want to filter the penguins dataset and apply an analysis only for observations for the species Adelie. Use .query() to filter the dataset. The query function is a powerful tool to filter datasets. Append .query() to the dataset and specify the condition you want to filter for.\n\n#Filter penguis dataset where species is Adelie\npenguins.query('species == \"Adelie\"').head()\n\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n5  Adelie  Torgersen            39.3  ...              190.0       3650.0    Male\n\n[5 rows x 7 columns]\n\n\nYou can also filter for multiple conditions. Say you want to filter for the species Adelie, but only if the sex of the penguins is not male.\n\n#Filter data with mulqitple conditions\npenguins.query('species == \"Adelie\" & sex != \"Male\" ').head()\n\n   species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n1   Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n2   Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n4   Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n6   Adelie  Torgersen            38.9  ...              181.0       3625.0  Female\n12  Adelie  Torgersen            41.1  ...              182.0       3200.0  Female\n\n[5 rows x 7 columns]"
  },
  {
    "objectID": "manipulation.html#arrange",
    "href": "manipulation.html#arrange",
    "title": "3  Data manipulation",
    "section": "3.2 Arrange",
    "text": "3.2 Arrange\nSometimes it is handy to sort the dataset. The .sort_values() function is used to sort the dataset. Append .sort_values() to the dataset and specify the variable you want to sort by.\n\n#Sort variables\npenguins.sort_values(by=\"body_mass_g\").head()\n\n       species     island  ...  body_mass_g     sex\n190  Chinstrap      Dream  ...       2700.0  Female\n64      Adelie     Biscoe  ...       2850.0  Female\n58      Adelie     Biscoe  ...       2850.0  Female\n116     Adelie  Torgersen  ...       2900.0  Female\n98      Adelie      Dream  ...       2900.0  Female\n\n[5 rows x 7 columns]\n\n\nIn other instances, you might want to sort the descendingly. The ascending parameter is used to sort the dataset in ascending order. By default, the ascending parameter is set to True. Set ascending to FALSE to sort the variable in descending order.\n\n#Sort variables descendingly\npenguins.sort_values(by=\"body_mass_g\", ascending=False).head()\n\n    species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g   sex\n237  Gentoo  Biscoe            49.2  ...              221.0       6300.0  Male\n253  Gentoo  Biscoe            59.6  ...              230.0       6050.0  Male\n297  Gentoo  Biscoe            51.1  ...              220.0       6000.0  Male\n337  Gentoo  Biscoe            48.8  ...              222.0       6000.0  Male\n299  Gentoo  Biscoe            45.2  ...              223.0       5950.0  Male\n\n[5 rows x 7 columns]"
  },
  {
    "objectID": "manipulation.html#select",
    "href": "manipulation.html#select",
    "title": "3  Data manipulation",
    "section": "3.3 Select",
    "text": "3.3 Select\nLarge datasets often contain many variables and are difficult to work with. The select function is used to select variables. For example, put the columns species and island in brackets to select them.\n\n#Select columns\npenguins[['species', 'island']].head()\n\n  species     island\n0  Adelie  Torgersen\n1  Adelie  Torgersen\n2  Adelie  Torgersen\n4  Adelie  Torgersen\n5  Adelie  Torgersen\n\n\nAnother way to select columns is to use the .loc function. The latter is used to access a group of rows and columns by labels or a boolean array. For example, select all columns from island to bill_depth_mm.\n\n#select columns with .loc: from island to bill_depth_mm\npenguins.loc[:, 'island':'bill_depth_mm'].head()\n\n      island  bill_length_mm  bill_depth_mm\n0  Torgersen            39.1           18.7\n1  Torgersen            39.5           17.4\n2  Torgersen            40.3           18.0\n4  Torgersen            36.7           19.3\n5  Torgersen            39.3           20.6\n\n\nYou can also select columns by their index. For example, select the first three columns of the dataset. Or suppose you want to check if a variable is in a specified set. Use the isin function to check if the variable is included in a set. For example, select the observations where the species is Adelie.\n\n#A set with the species Adelie\nmyset = ['Adelie']\n\n#Select observations where the species isin the set\nadelie = penguins[penguins[\"species\"].isin(myset)]\nadelie.head()\n\n  species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n5  Adelie  Torgersen            39.3  ...              190.0       3650.0    Male\n\n[5 rows x 7 columns]"
  },
  {
    "objectID": "manipulation.html#mutate",
    "href": "manipulation.html#mutate",
    "title": "3  Data manipulation",
    "section": "3.4 Mutate",
    "text": "3.4 Mutate\nThe mutate function is used to create new variables. We can use the assign function to create a new variable. For example, create a new variable weight which is a copy of the variable body_mass_g. Next, the .assign() function is used to create a new variable body_mass_kilo which is the body_mass_g divided by 1000. Feel free to manipulate the dataset and create new variables.\n\n# Create a new dataset\nweight = penguins[['body_mass_g']]\n\n#Assign a new variable by common operations\nweight = weight.assign(body_mass_kilo = weight['body_mass_g'] /1000)\nweight.head()\n\n   body_mass_g  body_mass_kilo\n0       3750.0            3.75\n1       3800.0            3.80\n2       3250.0            3.25\n4       3450.0            3.45\n5       3650.0            3.65"
  },
  {
    "objectID": "manipulation.html#summarize",
    "href": "manipulation.html#summarize",
    "title": "3  Data manipulation",
    "section": "3.5 Summarize",
    "text": "3.5 Summarize\nHow to calculate summary statistics of the dataset? The summarize function is used to calculate summary statistics of the dataset. The .describe() function is used to calculate summary statistics of the dataset.\nThe summarize function is used to calculate summary statistics of the dataset. The .describe() function is used to calculate summary statistics of the dataset.\n\n#Describe the dataset\npenguins.describe()\n\n       bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\ncount      333.000000     333.000000         333.000000   333.000000\nmean        43.992793      17.164865         200.966967  4207.057057\nstd          5.468668       1.969235          14.015765   805.215802\nmin         32.100000      13.100000         172.000000  2700.000000\n25%         39.500000      15.600000         190.000000  3550.000000\n50%         44.500000      17.300000         197.000000  4050.000000\n75%         48.600000      18.700000         213.000000  4775.000000\nmax         59.600000      21.500000         231.000000  6300.000000\n\n\nAnother way to calculate summary statistics is to use the .groupby() function. The .groupby function provides the power of the split-apply-combine pattern. The .groupby function is used to split the dataset into groups. The .apply function is used to apply a function to each group. The .combine function is used to combine the results into a new dataset. For example, calculate the mean body_mass_g of the penguins grouped by sex. Append .groupby() to the dataset and specify the variable you want to group by. In addition, use the .mean() function to calculate the mean of the grouped variable.\n\n# .groupby() and .mean() to calculate the mean body_mass_g of the penguins\npenguins[[\"sex\", \"body_mass_g\"]].groupby(\"sex\").mean()\n\n        body_mass_g\nsex                \nFemale  3862.272727\nMale    4545.684524\n\n\nThe .groupby function can be used to group by multiple variables. For example, calculate the mean body_mass_g of the penguins grouped by sex and species.\n\n#Add several variables to group by\npenguins.groupby([\"sex\", \"species\"])[\"body_mass_g\"].mean()\n\nsex     species  \nFemale  Adelie       3368.835616\n        Chinstrap    3527.205882\n        Gentoo       4679.741379\nMale    Adelie       4043.493151\n        Chinstrap    3938.970588\n        Gentoo       5484.836066\nName: body_mass_g, dtype: float64\n\n\nFinally, the .count() function is used to count the number of entries in each category of a variable.\n\n#shortr penguins[\"species\"].value_counts()\npenguins.groupby(\"species\")[\"species\"].count()\n\nspecies\nAdelie       146\nChinstrap     68\nGentoo       119\nName: species, dtype: int64\n\n\n\nSummary\nIn this script, we have shown how to use dplyr functions in Python. Keep in mind that the dplyr package is designed for data manipulation in R. However, the pandas library in Python provides similar functions to dplyr. Aggregation statistics can be calculated on entire columns or rows. The groupby function provides the power of the split-apply-combine pattern. The value_counts is a convenient shortcut to count the number of entries in each category of a variable.\nThe .loc function is used to access a group of rows and columns by labels or a boolean array. Say we want to create a indicator variable Adelie. The .loc function is used to create a new variable Adelie. The first line of code creates a new variable Adelie with the .loc function. The second line of code fills the rest of the column with False. The third line of code selects the columns species and Adelie.\n\n# Create a new variable with the .loc function\npenguins.loc[penguins['species'] == \"Adelie\", 'Adelie'] = 'True' \n\n#Fill the rest of the column with False\npenguins.loc[penguins['species'] != \"Adelie\", 'Adelie'] = 'False' \n\n#Select the columns species and Adelie\npenguins[['species', 'Adelie']].head()\n\n  species Adelie\n0  Adelie   True\n1  Adelie   True\n2  Adelie   True\n4  Adelie   True\n5  Adelie   True\n\n\nSo say we want to create a new variable check_length which checks if the bill_length_mm is less than or equal to 40. The .apply() function is used to apply a function along an axis of the dataframe. For example, the lambda function is used to create a new variable check_length. It checks if the bill_length_mm is less than or equal to 40. If not, the function returns False. If the condition is met, the function returns True.\n\n# Create a new variable check_length: check if the bill_length_mm is less than or equal to 40\npenguins['check_length'] = penguins['bill_length_mm'].apply(lambda x: 'True' if x &lt;= 40 else 'False')\n\n#Select the columns bill_length_mm and check_length\npenguins[['bill_length_mm', 'check_length']].head()\n\n   bill_length_mm check_length\n0            39.1         True\n1            39.5         True\n2            40.3        False\n4            36.7         True\n5            39.3         True"
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "4  Visualization",
    "section": "",
    "text": "5 Fonts\nTo use a custom font in a plot, you need to download the font file and specify the path to the font file. You can download a font file from a website like Google Fonts. For example, to download the Roboto font from Google Fonts, use the following code.\nimport urllib.request\n\nurl = 'https://fonts.googleapis.com/css2?family=Roboto'  \n# Replace with the Google Fonts URL\nfont_path = 'Roboto-Regular.ttf'\n\nurllib.request.urlretrieve(url, font_path)\nNext, you need to specify the path to the font file and set the font family in the plot. The following code shows how to use the Roboto font in a plot.\nimport matplotlib.font_manager as fm\n\nfont_path = 'Roboto-Regular.ttf'  # Path to your downloaded font\nfont_prop = fm.FontProperties(fname=font_path)\nplt.rcParams['font.family'] = font_prop.get_name()\n\nplot = (\n    ggplot(data, aes('x', 'y')) +\n    geom_point() +\n    theme(\n        plot_title=element_text(family=font_prop.get_name(), size=14, weight='bold')\n    )\n)"
  },
  {
    "objectID": "visualization.html#categorical-data",
    "href": "visualization.html#categorical-data",
    "title": "4  Visualization",
    "section": "4.1 Categorical data",
    "text": "4.1 Categorical data\nAs outlined, the searborn package is built on top of matplotlib and provides a high-level interface for creating attractive visualizations. The seaborn package provides a number of functions for creating visualizations of categorical data. For example, you can create bar graphs, box plots, and violin plots with the seaborn package. To create a bar graph of the bill length of the penguins grouped by species, append the .barplot() function to the sns object and specify the data, the x variable, and the y variable.\n\n#Bar graph\nsns.barplot(data=penguins, x=\"species\", y=\"bill_length_mm\")\n\n\n\n\nThe catplot() function can be used to create a variety of visualizations of categorical data. For example, you can create a box plot of the bill length of the penguins grouped by species with the catplot() function. To create a box plot, set the kind parameter to “box”.\n\n#Make a box plot with the catplot() function\nsns.catplot(data=penguins, kind=\"box\", x=\"sex\", y=\"bill_length_mm\")\n\n\nBox plots are a good way to visualize the distribution of a continuous variable within different categories, but they do not show the individual data points. A swarm plot is a good way to visualize the individual data points within different categories. A swarm plot is similar to a scatter plot, but it arranges the data points so that they do not overlap. To create a swarm plot of the bill length of the penguins grouped by species, set the kind parameter to “swarm”.\n\n#Make a swarm plot\nsns.catplot(data=penguins, kind=\"swarm\", x=\"sex\", y=\"bill_length_mm\", hue=\"species\")"
  },
  {
    "objectID": "visualization.html#relationsships",
    "href": "visualization.html#relationsships",
    "title": "4  Visualization",
    "section": "4.2 Relationsships",
    "text": "4.2 Relationsships\nNext, we will focus on histograms and scatter plots. These are common visualizations for exploring numerical variables and the relationship between two continuous variables. To create a histogram of the body mass of the penguins, use the hist() function from the matplotlib package. The hist() function takes the data as input and creates a histogram of the data. The number of bins can be specified with the bins parameter.\n\n#Hist with matplotlib\nplt.hist(penguins.body_mass_g, bins=30)\n\n\n\n\nAnd in case of a scatter plot, you can use the scatter() function from the matplotlib package. The scatter() function takes the x-axis variable and the y-axis variable as input and creates a scatter plot of the data. For example, how is the body mass of the penguins related to the bill length?\n\n#Scatterplot with matplotlib\nplt.scatter(penguins.body_mass_g, penguins.bill_length_mm)\n\n\n\n\nComing from the R universe, I don’t like the default style of matplotlib. I prefer the ggplot style. To use the ggplot style, you can use the matplotlib.style.use() function and set the style to “ggplot”.\n\n#Adjust the style\nmatplotlib.style.use('ggplot')\nplt.scatter(penguins.body_mass_g, penguins.bill_length_mm)\n\n\n\n\nNobody said that the ggplot style is the best. It’s just a matter of taste and I also do not like some details of default ggplot2 style. The good news is, we can adjust all details if we switch back to the ggplot2 approach and the plotnine package. Having an R background, the code to create the same scatter plot with the plotnine package looks very familiar. The ggplot() function creates a plot object; the aes() function is used to specify the x-axis variable and the y-axis variable. The geom_point() function adds points to the plot or more precisely, it adds a geom to the plot. The stat_smooth() function is used to add a linear regression line to the plot. The facet_wrap() function is used to create separate plots for each species.\n\n#Scatterplot with plotnine\nfrom plotnine import *\n\n#Create a scatter plot with plotnine\np = ggplot(penguins, aes('body_mass_g', 'bill_length_mm')) + geom_point()\n\n#Add a linear regression line and facet the plot by species\np = p + stat_smooth(method='lm') + facet_wrap('~species')\n \nprint(p)\n\n\n\n\nLook at the next code: We can create a box plot of the body mass of the penguins grouped by species with the geom_boxplot() function. The factor() function is used to convert the species variable to a factor variable. In addition, themes in plotnine can be adjusted like in ggplot2. The theme_dark() function creates a dark theme for the plot or the theme_minimal() function creates a minimal theme for the plot.\n\n# A boxplot with factor variables and a theme\np = ggplot(penguins) + geom_boxplot(aes(x='factor(species)', y='body_mass_g')) + theme_minimal()\n\nprint(p)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ushey, Kevin, JJ Allaire, and Yuan Tang. 2023. “Reticulate:\nInterface to ’Python’.” https://CRAN.R-project.org/package=reticulate.\n\n\nWaskom, Michael L. 2021. “Seaborn: Statistical Data\nVisualization.” Journal of Open Source Software 6 (60):\n3021. https://doi.org/10.21105/joss.03021."
  }
]