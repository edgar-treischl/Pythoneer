[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pythoneer",
    "section": "",
    "text": "Preface\nThe Pythoneer book is work in progress and at a very early stage. The goal is to provide a guide for R users to transition into Python. The project is open-source and contributions are welcome. There are several reasons why it can be beneficial for R users to learn Python:\nThis chapter aims to provide you with a brief overview of the book. We will discuss the motivation behind this book, the target audience, and the structure of the book. Formost, I try to outline how you can transfer your R knowledge to Python.\nWhat is my motivation to write this book? I tried to learn Python with the help of several books and classes. Coding and programming languages were not new for me, but still I failed to learn Python. I consider myself a tech-savvy without having a heavy programming background, but a lot of fun learning new things and enjoy coding. For this reason I started this book. If you want to learn Python to automate processes, apply statistical procedures from the scratch, or use Python to program games, this book will not help you in any way. However, you do wanna transfer your R knowledge, this book might be right choice. Thus I assume you have some basic knowledge of R and programming skills and there is no need to explain basic concepts such as loops, functions, or data types.\n#Print with Python\nprint(\"👋 World\")\n\n#&gt; 👋 World\nLet be give you an example why I started this book",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#how-not-to-calculate-basic-statistics",
    "href": "index.html#how-not-to-calculate-basic-statistics",
    "title": "Pythoneer",
    "section": "How (not) to calculate basic statistics",
    "text": "How (not) to calculate basic statistics\nBefore we can do anything, we have to install Python, learn how to manage virtual environments, and we how we can install packages. As with R, you have to install and import libraries for most tasks. We will cover these steps in the next chapter. For now, we will focus on some code snippets only. Say we use the seaborn package because it provides us with some data. We can use the mpg data set as cars and fool around with it. Only if you have never seen the mpg data set, you can use the following code snippet to get an overview of the data set.\n\n#import seaborn to get some data\nimport seaborn as sns\n\n#load the mpg data set as cars\ncars = sns.load_dataset(\"mpg\") \nprint(cars)\n\n#&gt;       mpg  cylinders  ...  origin                       name\n#&gt; 0    18.0          8  ...     usa  chevrolet chevelle malibu\n#&gt; 1    15.0          8  ...     usa          buick skylark 320\n#&gt; 2    18.0          8  ...     usa         plymouth satellite\n#&gt; 3    16.0          8  ...     usa              amc rebel sst\n#&gt; 4    17.0          8  ...     usa                ford torino\n#&gt; ..    ...        ...  ...     ...                        ...\n#&gt; 393  27.0          4  ...     usa            ford mustang gl\n#&gt; 394  44.0          4  ...  europe                  vw pickup\n#&gt; 395  32.0          4  ...     usa              dodge rampage\n#&gt; 396  28.0          4  ...     usa                ford ranger\n#&gt; 397  31.0          4  ...     usa                 chevy s-10\n#&gt; \n#&gt; [398 rows x 9 columns]\n\n\nSuppose a mean function is not implemented in Python. We can create our own function to calculate the mean. We need to define a function that calculates the mean of an array. First, we have to define the function with the def keyword, followed by the name of the function and the input value. The function should return the sum of the array divided by the length of the array.\n\n# Define a function that calculates the mean of an array\ndef mean (array): \n    n = len(array)\n    return sum(array) / n\n\n#Apply the function to the mpg column of the cars data set\nmean(cars.mpg)\n\n#&gt; 23.514572864321615\n\n\nThat’s a cool way to show us how a function works. We have to provide a name of a function and tell Python what the function does. I always learned how functions and other concepts work in much more artificial way, especially in classes courses about Python. Look how I learned how a function works in Python:\n\n#Create a funny function\ndef hello(name):\n    return (f\"Hallo, {name}! How are you?\")\n\n# Apply the funny function\nhello(\"Edgar\")\n\n#&gt; 'Hallo, Edgar! How are you?'\n\n\nThus, we create the hello function that returns a sentence and inserts the name of the input value. Nothing wrong about that, even calculating a mean seems a little bit more realistic to illustrate why we need such a function. Anyway, in some classes I learned how to calculate the mean, the median, and the modus. Maybe a function to calculate the variance or others measure of central tendency. You know what Pearson R is? Guess what the next code does? And I guess you will skip the code after line 2, well that’s what recommend since the code only illustrates my point. The next console shows how to calculate the correlation coefficient between two arrays and I actually copied to code from a book.\n\n# Define a function that calculates the correlation coefficient\n\nimport math\ndef correlation(x, y):\n    n = len(x) \n    \n    # Means\n    x_mn = sum(x) / n \n    y_mn = sum(y) / n\n    \n    # Variance\n    var_x = (1 / (n-1)) * sum(map(lambda xi: (xi - x_mn) ** 2 , x)) \n    var_y = (1 / (n-1)) * sum(map(lambda yi: (yi - y_mn) ** 2 , y))\n    \n    # Std\n    std_x, std_y = math.sqrt(var_x), math.sqrt(var_y)\n    \n    # Covariance\n    xy_var = map(lambda xi, yi: (xi - x_mn) * (yi - y_mn), x, y) \n    cov = (1 / (n-1)) * sum(xy_var)\n    \n    # Pearson's R\n    r = cov / (std_x * std_y) \n    return float(f\"{r:.3f}\")\n\n# Some data\nsize = [20, 15, 40, 25, 35]\ncost = [300, 400, 600, 700, 666]\n\nprint(correlation(size, cost))\n\n#&gt; 0.666\n\n\nPlease, use numpy to get the scientific toolkit and pandas for tabular processing and the presentation of data. The word numpy stands for numerical Python and is a package that provides us with a lot of functions to work with arrays. The pandas package is a data manipulation and analysis library that provides us with data structures and functions to manipulate data. We can use the numpy package to calculate the correlation coefficient between two arrays. The numpy package provides us with the corrcoef function that takes two arrays as input and returns the correlation coefficient.\n\n# corrcoef function from numpy (short: np): \nimport numpy as np\nnp.corrcoef(size, cost)\n\n#&gt; array([[1.        , 0.66645893],\n#&gt;        [0.66645893, 1.        ]])\n\n\nThe pandas package provides us with the describe function that returns an overview of the data. We can use the mean and std functions to calculate the mean and standard deviation of the data.\n\n# describe function from pandas (short: pd)\nimport pandas as pd\n\n# describe cars\ncars.describe()\n\n#&gt;               mpg   cylinders  ...  acceleration  model_year\n#&gt; count  398.000000  398.000000  ...    398.000000  398.000000\n#&gt; mean    23.514573    5.454774  ...     15.568090   76.010050\n#&gt; std      7.815984    1.701004  ...      2.757689    3.697627\n#&gt; min      9.000000    3.000000  ...      8.000000   70.000000\n#&gt; 25%     17.500000    4.000000  ...     13.825000   73.000000\n#&gt; 50%     23.000000    4.000000  ...     15.500000   76.000000\n#&gt; 75%     29.000000    8.000000  ...     17.175000   79.000000\n#&gt; max     46.600000    8.000000  ...     24.800000   82.000000\n#&gt; \n#&gt; [8 rows x 7 columns]\n\n\nWe have to append the describe() function to the saved cars data in order to get an overview of the central tendency measures. You can do the same with all other functions that calculate other measures, such as the mean or standard deviance:\n\n#append the mean and std function to the cars data set\ncars.mean()\n\n#&gt; mpg               23.514573\n#&gt; cylinders          5.454774\n#&gt; displacement     193.425879\n#&gt; horsepower       104.469388\n#&gt; weight          2970.424623\n#&gt; acceleration      15.568090\n#&gt; model_year        76.010050\n#&gt; dtype: float64\n\ncars.std()\n\n#&gt; mpg               7.815984\n#&gt; cylinders         1.701004\n#&gt; displacement    104.269838\n#&gt; horsepower       38.491160\n#&gt; weight          846.841774\n#&gt; acceleration      2.757689\n#&gt; model_year        3.697627\n#&gt; dtype: float64\n\n\nAnd we should at least look at the scatter plot since we talk about correlations. The matplotlib provides a lot of different graphs for us. And in case you are an R user, you can even use the ggplot2 style, just to let you show some possibilities. But of course, we have to think more systematically how we can reach our goal in the next chapter.\n\n#scatter plot example\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.style.use('ggplot')\n\nplt.scatter(cars.mpg, cars.horsepower)\n\n\n\n\n\n\n\n\n\nSummary\nThis chapter provided you with a brief overview of the book. We discussed the motivation behind this book, the target audience, and the structure of the book. We also outlined how you can transfer your R knowledge to Python. In the next chapter, we will discuss how to install Python, manage virtual environments, and install packages.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "firststeps.html",
    "href": "firststeps.html",
    "title": "1  First steps",
    "section": "",
    "text": "Python and R are both popular programming languages used for data analysis and machine learning tasks. While both languages have their own strengths and weaknesses, there are situations where using both languages together can be beneficial.\nUsing Python and R together can be beneficial when you want to take advantage of the strengths of both languages. Whether you want to use R libraries in Python, Python libraries in R, or combine both languages in a single application, there are many options available for integrating Python and R. Here are some ways in which Python can be used with R:\n\nR can be called from Python using the “rpy2” package: rpy2 is a Python package that allows Python programs to call R functions and use R data structures. This can be useful when you want to use an R library that is not available in Python or when you have existing R code that you want to integrate into your Python program.\nJupyter Notebooks can be used to combine Python and R code: Jupyter Notebooks are interactive documents that allow you to combine code, text, and visualizations in a single document. Jupyter Notebooks support both Python and R, so you can use both languages in the same notebook.\nShiny applications can be built using Python: Shiny is a popular web framework for building interactive web applications in R. However, Shiny also supports using Python through the “reticulate” package. This can be useful when you want to use a Python library that is not available in R or when you have existing Python code that you want to integrate into your Shiny application.\nFinally, Python can be called from R using the reticulate package: Reticulate is a package that allows R programs to call Python functions and use Python data structures. This can be useful when you want to use a Python library that is not available in R or when you have existing Python code that you want to integrate into your R program (Ushey, Allaire, and Tang 2023).\n\nThe reticulate package makes it easy for R users to incorporate Python functionality into their R workflows, whether it’s using Python libraries not available in R, reusing existing Python code, or simply taking advantage of Python’s strengths in specific areas such as machine learning or web development.\n\nPython environment management: The package provides tools to manage Python environments within R, including creating and configuring virtual environments and managing package dependencies.\nCalling Python code: The package provides functions to call Python code from R, including importing Python modules, calling Python functions, and accessing Python objects. Passing data between R and Python: The package allows for seamless integration between R and Python data structures. For example, R data frames can be converted to Python pandas data frames and vice versa.\nInteractive sessions: The package provides an interactive Python console within R sessions, allowing users to execute Python commands interactively.\nPlotting: The package allows R users to create Python plots using popular Python visualization libraries such as Matplotlib and Seaborn.\n\nFirst, you need to have Python installed on your system. You can download Python from the official website (https://www.python.org/downloads/) and install it following the instructions provided. Next, install the reticulate package: Open an R session and install the reticulate package by running the following command:\n\n# install reticulate package\ninstall.packages(\"reticulate\")\n\nOnce the package is installed, you need to set the Python path in R. The Python path is the location of the Python executable on your system. You can set the path by running the following command:\n\nlibrary(reticulate)\nuse_python(\"/Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/bin/python\")\n\nIf you’re using the default Python installation on your system, you can skip this step. You can test the installation by running the following command:\n\npy_config()\n\n#&gt; python:         /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/bin/python\n#&gt; libpython:      /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/lib/libpython3.8.dylib\n#&gt; pythonhome:     /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate:/Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate\n#&gt; version:        3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:13)  [Clang 14.0.6 ]\n#&gt; numpy:          /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/lib/python3.8/site-packages/numpy\n#&gt; numpy_version:  1.24.2\n#&gt; \n#&gt; NOTE: Python version was forced by use_python() function\n\n\nThis command should display information about your Python installation, including the Python version, the location of the Python executable, and the Python library path. Once the reticulate package is installed and configured, you can start using Python functionality in your R code. For example, you can import Python modules, call Python functions, and access Python objects directly from R.\nHowever, I’ll use a virtual environment for the book and the conda_list() functions returns all (conda) environment of the system.\n\n#List all conda installations\nconda_list()\n\n#&gt;             name\n#&gt; 1    .PreTrained\n#&gt; 2           base\n#&gt; 3       EdgarGPT\n#&gt; 4      IliartBot\n#&gt; 5 IliartGPT.venv\n#&gt; 6          flask\n#&gt; 7         fuckit\n#&gt; 8   r-reticulate\n#&gt; 9 streamlit.venv\n#&gt;                                                                  python\n#&gt; 1         /Users/edgar/Dropbox/Python/PreTrained/.PreTrained/bin/python\n#&gt; 2                     /Users/edgar/Library/r-miniconda-arm64/bin/python\n#&gt; 3       /Users/edgar/Library/r-miniconda-arm64/envs/EdgarGPT/bin/python\n#&gt; 4      /Users/edgar/Library/r-miniconda-arm64/envs/IliartBot/bin/python\n#&gt; 5 /Users/edgar/Library/r-miniconda-arm64/envs/IliartGPT.venv/bin/python\n#&gt; 6          /Users/edgar/Library/r-miniconda-arm64/envs/flask/bin/python\n#&gt; 7         /Users/edgar/Library/r-miniconda-arm64/envs/fuckit/bin/python\n#&gt; 8   /Users/edgar/Library/r-miniconda-arm64/envs/r-reticulate/bin/python\n#&gt; 9 /Users/edgar/Library/r-miniconda-arm64/envs/streamlit.venv/bin/python\n\n\nVirtual environments in Python are a useful tool for managing dependencies and ensuring that your project runs smoothly on different systems. Here are several reasons why it is wise to use virtual environments in Python:\n\nIsolation: When you create a virtual environment, you create a self-contained Python environment with its own installation of Python and any required packages. This means that the packages installed in one virtual environment do not interfere with packages installed in other virtual environments or the global Python environment. Isolating packages in this way reduces the risk of package version conflicts and makes it easier to manage dependencies.\nReproducibility: By using virtual environments, you can ensure that your code runs in a consistent and reproducible environment, regardless of the system it is run on. This is especially important if you plan to share your code with others or if you need to run your code on multiple systems.\nFlexibility: Virtual environments make it easy to switch between different Python versions or package configurations. This can be useful if you need to work on multiple projects that require different versions of Python or different package dependencies.\nSecurity: When you install packages in a virtual environment, you are not affecting the global Python installation on your system. This means that any security vulnerabilities or issues with the packages you install are contained within the virtual environment, reducing the risk of affecting other parts of your system.\n\nCreate a Python environment using the virtualenv_create() function. This function creates a new virtual environment and installs the specified packages. For example, to create a virtual environment called “myenv” and install the pandas package, you can run the following command:\n\nvirtualenv_create(\"myenv\", packages = \"pandas\")\n\nFinally, you can install additional Python packages using the py_install() function. This function installs the specified packages in the active Python environment. For example, to install the pandas package, you can run the following command:\n\npy_install(\"pandas\")\n\nThe next console achieves the same result with Conda and import the Pandas module.\n\n#install packages\nconda_install(\"r-reticulate\", \"pandas\")\npandas &lt;- import(\"pandas\")\n\nLet us explore some Python packages to see how we can intregrate them in R. For example, Seaborn is a visualization library based on Matplotlib. It provides a high-level interface for creating informative and attractive statistical graphics. It even gives us access to know data sets such as mtcars or Anscombe’s quartet.\nAnscombe’s quartet is a set of four datasets that have nearly identical descriptive statistics, yet have very different plots when graphed. The quartet was created by the statistician Francis Anscombe in 1973 to demonstrate the importance of visualizing data and the limitations of summary statistics. Each dataset consists of 11 (x, y) pairs, and the four datasets are designed to have the same mean, variance, correlation, and linear regression line. However, when plotted, each dataset reveals a very different relationship between x and y. The quartet is often used to illustrate the importance of data visualization and exploratory data analysis in understanding and interpreting statistical results.\n\nimport seaborn as sns\nsns.set_theme(style=\"ticks\")\n\n# Load the example dataset for Anscombe's quartet\ndf = sns.load_dataset(\"anscombe\")\n\n# Show the results of a linear regression within each dataset\nsns.lmplot(\n    data=df, x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\",\n    col_wrap=2, palette=\"muted\", ci=None,\n    height=4, scatter_kws={\"s\": 50, \"alpha\": 1}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet us load the “mpg” dataset and display the first few rows of the data. If you haven’t already, you’ll need to install Seaborn first (Waskom 2021).\nLoad Seaborn and the “mpg” dataset: Once you have Seaborn installed, you can load it and the “mpg” dataset using the following Python code. It imports the Seaborn package as “sns” and loads the “mpg” dataset into a variable called “cars”. Moreover, I also import the Pandas package which we will use as well.\n\n#import numpy as np\n#import pandas as pd\nimport seaborn as sns\nimport pandas as pd\n\ncars = sns.load_dataset(\"mpg\")\n\nTo display the first few rows of the “mpg” dataset, you can use the head() function from the Pandas library, which is included in the Seaborn package:\n\n#print(cars)\ncars.head()\n\n#&gt;     mpg  cylinders  displacement  ...  model_year  origin                       name\n#&gt; 0  18.0          8         307.0  ...          70     usa  chevrolet chevelle malibu\n#&gt; 1  15.0          8         350.0  ...          70     usa          buick skylark 320\n#&gt; 2  18.0          8         318.0  ...          70     usa         plymouth satellite\n#&gt; 3  16.0          8         304.0  ...          70     usa              amc rebel sst\n#&gt; 4  17.0          8         302.0  ...          70     usa                ford torino\n#&gt; \n#&gt; [5 rows x 9 columns]\n\n\nThe Pandas library is a popular data analysis toolkit for Python. It provides a wide range of functions and tools for working with structured data. For example, the describe() function is used to generate descriptive statistics of a Pandas DataFrame or Series. When called on a DataFrame or Series, the describe() function provides summary statistics such as count, mean, standard deviation, minimum, maximum, and quartiles. The next console shows how to generate descriptive statistics for the “mpg” dataset using the describe() function, you can use the following Python code:\n\ncars.describe()\n\n#&gt;               mpg   cylinders  ...  acceleration  model_year\n#&gt; count  398.000000  398.000000  ...    398.000000  398.000000\n#&gt; mean    23.514573    5.454774  ...     15.568090   76.010050\n#&gt; std      7.815984    1.701004  ...      2.757689    3.697627\n#&gt; min      9.000000    3.000000  ...      8.000000   70.000000\n#&gt; 25%     17.500000    4.000000  ...     13.825000   73.000000\n#&gt; 50%     23.000000    4.000000  ...     15.500000   76.000000\n#&gt; 75%     29.000000    8.000000  ...     17.175000   79.000000\n#&gt; max     46.600000    8.000000  ...     24.800000   82.000000\n#&gt; \n#&gt; [8 rows x 7 columns]\n\n\nOr consider the mean() function which is used to calculate the arithmetic mean of a Pandas DataFrame or Series. When called on a DataFrame or Series, the mean() function calculates the average value of all the elements in the DataFrame or Series.\n\ncars.mean()\n\n#&gt; mpg               23.514573\n#&gt; cylinders          5.454774\n#&gt; displacement     193.425879\n#&gt; horsepower       104.469388\n#&gt; weight          2970.424623\n#&gt; acceleration      15.568090\n#&gt; model_year        76.010050\n#&gt; dtype: float64\n\n\nScipy is a popular library for scientific computing in Python, and it provides functions for calculating statistical values, such as the correlation coefficient. Start by importing the necessary packages. You’ll need the scipy.stats module to calculate the correlation coefficient, and you may also need numpy to work with arrays or matrices of data.\n\nimport scipy\nimport scipy.stats as stats\nimport numpy as np\n\nThe scipy.stats module can calculate the correlation coefficient for two arrays, but if you have more than two variables, you’ll need to use a matrix. Use the stats.pearsonr() function to calculate the Pearson correlation coefficient. This function takes two arguments: the two arrays or matrices to compare, and it returns two values: the correlation coefficient and the p-value.\n\nmpg = cars[\"mpg\"]\nhorsepower = cars[\"horsepower\"]\n\nscipy.stats.pearsonr(mpg, horsepower)\n\n#&gt; ValueError: array must not contain infs or NaNs\n\n\nUnfortuntely, there is a missing values problem that we need to fix first. In Python, missing values are typically represented by the special value NaN (Not a Number), which is part of the numpy library. There are different ways to drop missing values from a dataset, depending on the context and the desired outcome.\nDrop rows or columns with missing values: If you have missing values in your dataset and you want to remove entire rows or columns that contain at least one missing value, you can use the dropna() method. This method removes all rows that contain at least one missing value by default, but you can specify the argument axis=1 to remove columns instead.\nIf you want to drop missing values only within a specific column, you can use the dropna() method with the subset argument. This argument specifies the column or columns to consider when dropping missing values. The next console shows the first approaches and the results of the correlation coeficient.\n\ncars = cars.dropna()\nmpg = cars[\"mpg\"]\nhorsepower = cars[\"horsepower\"]\n\nscipy.stats.pearsonr(mpg, horsepower)\n\n#&gt; PearsonRResult(statistic=-0.7784267838977761, pvalue=7.031989029403436e-81)\n\n\nAlternatively, if you have a pandas DataFrame, you can use the pandas.DataFrame.corr() method to calculate the correlation coefficient for all pairs of columns.\n\ncorr_matrix = cars.corr(method='pearson')\nprint(corr_matrix)\n\n#&gt;                    mpg  cylinders  ...  acceleration  model_year\n#&gt; mpg           1.000000  -0.777618  ...      0.423329    0.580541\n#&gt; cylinders    -0.777618   1.000000  ...     -0.504683   -0.345647\n#&gt; displacement -0.805127   0.950823  ...     -0.543800   -0.369855\n#&gt; horsepower   -0.778427   0.842983  ...     -0.689196   -0.416361\n#&gt; weight       -0.832244   0.897527  ...     -0.416839   -0.309120\n#&gt; acceleration  0.423329  -0.504683  ...      1.000000    0.290316\n#&gt; model_year    0.580541  -0.345647  ...      0.290316    1.000000\n#&gt; \n#&gt; [7 rows x 7 columns]\n\n\nIn the next two chapter we learn more about data preparation, analysis, and visualization with Python. For example, the statsmodels.formula.api module is used to create and fit a linear regression model. The formula for the regression model is defined using a string that specifies the dependent variable and the independent variables, separated by ~ operator which is very similar compared to R. The ols() method is used to create the model, passing in the formula and the dataset. Finally, the fit() method is used to fit the model to the data, and the summary() method is used to print a summary of the model results.\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\nresults = smf.ols('mpg ~ horsepower', data=cars).fit()\nprint(results.summary())\n\n#&gt;                             OLS Regression Results                            \n#&gt; ==============================================================================\n#&gt; Dep. Variable:                    mpg   R-squared:                       0.606\n#&gt; Model:                            OLS   Adj. R-squared:                  0.605\n#&gt; Method:                 Least Squares   F-statistic:                     599.7\n#&gt; Date:                Tue, 26 Aug 2025   Prob (F-statistic):           7.03e-81\n#&gt; Time:                        15:50:14   Log-Likelihood:                -1178.7\n#&gt; No. Observations:                 392   AIC:                             2361.\n#&gt; Df Residuals:                     390   BIC:                             2369.\n#&gt; Df Model:                           1                                         \n#&gt; Covariance Type:            nonrobust                                         \n#&gt; ==============================================================================\n#&gt;                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n#&gt; ------------------------------------------------------------------------------\n#&gt; Intercept     39.9359      0.717     55.660      0.000      38.525      41.347\n#&gt; horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n#&gt; ==============================================================================\n#&gt; Omnibus:                       16.432   Durbin-Watson:                   0.920\n#&gt; Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\n#&gt; Skew:                           0.492   Prob(JB):                     0.000175\n#&gt; Kurtosis:                       3.299   Cond. No.                         322.\n#&gt; ==============================================================================\n#&gt; \n#&gt; Notes:\n#&gt; [1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nUshey, Kevin, JJ Allaire, and Yuan Tang. 2023. “Reticulate: Interface to ’Python’.” https://CRAN.R-project.org/package=reticulate.\n\n\nWaskom, Michael L. 2021. “Seaborn: Statistical Data Visualization.” Journal of Open Source Software 6 (60): 3021. https://doi.org/10.21105/joss.03021.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>First steps</span>"
    ]
  },
  {
    "objectID": "manipulation.html",
    "href": "manipulation.html",
    "title": "2  Data manipulation",
    "section": "",
    "text": "2.1 Filter\nWhat is dplyr? The dplyr package is designed for data manipulation, written and maintained by Hadley Wickham. It is part of the tidyverse, an ecosystem of packages designed with common APIs and a shared philosophy. Dplyr is designed to abstract common data manipulation tasks into a consistent API. However, the pandas library provides similar functions. The pandas package is a powerful data manipulation library in Python. It provides data structures and functions to manipulate data and in this chapter we will see how to use dplyr functions work in Python.\nFirst, we need to import the necessary libraries. We will use pandas and seaborn. Furthermore, we will load the penguins dataset with load_dataset from seaborn. Let us have a look at the first five rows of the dataset with the .head function.\nHow does the dataset look like? The .head() function is used to show the first five rows of the dataset. The .tail() function is used to show the last five rows of the dataset. The .shape function is used to show the dimensions of the dataset. Overall, the penguins dataset contains 333 entries and 7 columns. The columns are for example species, island, and body_mass\nMaybe you did not realize it, but the penguins dataset contains missing values. The dropna function is used to remove missing values from the dataset. And we can append the .info() function to get an overview of the dataset. The .info() function shows the number of non-null entries in each column. The dataset contains 333 entries and 7 columns.\nIf you want to know the type of the variables in the dataset, you can use the .dtypes function. The dtypes function shows the data types of the variables. In this dataset, the variables species, island, and sex are of type object. Or variable bill_length_mm is of type float64.\nA common task in data manipulation is to filter the dataset. Suppose you want to filter the penguins dataset and apply an analysis only for observations for the species Adelie. Use .query() to filter the dataset. The query function is a powerful tool to filter datasets. Append .query() to the dataset and specify the condition you want to filter for.\n#Filter penguis dataset where species is Adelie\npenguins.query('species == \"Adelie\"').head()\n\n#&gt;   species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n#&gt; 0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n#&gt; 1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n#&gt; 2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n#&gt; 4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n#&gt; 5  Adelie  Torgersen            39.3  ...              190.0       3650.0    Male\n#&gt; \n#&gt; [5 rows x 7 columns]\nYou can also filter for multiple conditions. Say you want to filter for the species Adelie, but only if the sex of the penguins is not male.\n#Filter data with mulqitple conditions\npenguins.query('species == \"Adelie\" & sex != \"Male\" ').head()\n\n#&gt;    species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n#&gt; 1   Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n#&gt; 2   Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n#&gt; 4   Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n#&gt; 6   Adelie  Torgersen            38.9  ...              181.0       3625.0  Female\n#&gt; 12  Adelie  Torgersen            41.1  ...              182.0       3200.0  Female\n#&gt; \n#&gt; [5 rows x 7 columns]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#arrange",
    "href": "manipulation.html#arrange",
    "title": "2  Data manipulation",
    "section": "2.2 Arrange",
    "text": "2.2 Arrange\nSometimes it is handy to sort the dataset. The .sort_values() function is used to sort the dataset. Append .sort_values() to the dataset and specify the variable you want to sort by.\n\n#Sort variables\npenguins.sort_values(by=\"body_mass_g\").head()\n\n#&gt;        species     island  ...  body_mass_g     sex\n#&gt; 190  Chinstrap      Dream  ...       2700.0  Female\n#&gt; 64      Adelie     Biscoe  ...       2850.0  Female\n#&gt; 58      Adelie     Biscoe  ...       2850.0  Female\n#&gt; 116     Adelie  Torgersen  ...       2900.0  Female\n#&gt; 98      Adelie      Dream  ...       2900.0  Female\n#&gt; \n#&gt; [5 rows x 7 columns]\n\n\nIn other instances, you might want to sort the descendingly. The ascending parameter is used to sort the dataset in ascending order. By default, the ascending parameter is set to True. Set ascending to FALSE to sort the variable in descending order.\n\n#Sort variables descendingly\npenguins.sort_values(by=\"body_mass_g\", ascending=False).head()\n\n#&gt;     species  island  bill_length_mm  ...  flipper_length_mm  body_mass_g   sex\n#&gt; 237  Gentoo  Biscoe            49.2  ...              221.0       6300.0  Male\n#&gt; 253  Gentoo  Biscoe            59.6  ...              230.0       6050.0  Male\n#&gt; 297  Gentoo  Biscoe            51.1  ...              220.0       6000.0  Male\n#&gt; 337  Gentoo  Biscoe            48.8  ...              222.0       6000.0  Male\n#&gt; 299  Gentoo  Biscoe            45.2  ...              223.0       5950.0  Male\n#&gt; \n#&gt; [5 rows x 7 columns]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#select",
    "href": "manipulation.html#select",
    "title": "2  Data manipulation",
    "section": "2.3 Select",
    "text": "2.3 Select\nLarge datasets often contain many variables and are difficult to work with. The select function is used to select variables. For example, put the columns species and island in brackets to select them.\n\n#Select columns\npenguins[['species', 'island']].head()\n\n#&gt;   species     island\n#&gt; 0  Adelie  Torgersen\n#&gt; 1  Adelie  Torgersen\n#&gt; 2  Adelie  Torgersen\n#&gt; 4  Adelie  Torgersen\n#&gt; 5  Adelie  Torgersen\n\n\nAnother way to select columns is to use the .loc function. The latter is used to access a group of rows and columns by labels or a boolean array. For example, select all columns from island to bill_depth_mm.\n\n#select columns with .loc: from island to bill_depth_mm\npenguins.loc[:, 'island':'bill_depth_mm'].head()\n\n#&gt;       island  bill_length_mm  bill_depth_mm\n#&gt; 0  Torgersen            39.1           18.7\n#&gt; 1  Torgersen            39.5           17.4\n#&gt; 2  Torgersen            40.3           18.0\n#&gt; 4  Torgersen            36.7           19.3\n#&gt; 5  Torgersen            39.3           20.6\n\n\nYou can also select columns by their index. For example, select the first three columns of the dataset. Or suppose you want to check if a variable is in a specified set. Use the isin function to check if the variable is included in a set. For example, select the observations where the species is Adelie.\n\n#A set with the species Adelie\nmyset = ['Adelie']\n\n#Select observations where the species isin the set\nadelie = penguins[penguins[\"species\"].isin(myset)]\nadelie.head()\n\n#&gt;   species     island  bill_length_mm  ...  flipper_length_mm  body_mass_g     sex\n#&gt; 0  Adelie  Torgersen            39.1  ...              181.0       3750.0    Male\n#&gt; 1  Adelie  Torgersen            39.5  ...              186.0       3800.0  Female\n#&gt; 2  Adelie  Torgersen            40.3  ...              195.0       3250.0  Female\n#&gt; 4  Adelie  Torgersen            36.7  ...              193.0       3450.0  Female\n#&gt; 5  Adelie  Torgersen            39.3  ...              190.0       3650.0    Male\n#&gt; \n#&gt; [5 rows x 7 columns]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#mutate",
    "href": "manipulation.html#mutate",
    "title": "2  Data manipulation",
    "section": "2.4 Mutate",
    "text": "2.4 Mutate\nThe mutate function is used to create new variables. We can use the assign function to create a new variable. For example, create a new variable weight which is a copy of the variable body_mass_g. Next, the .assign() function is used to create a new variable body_mass_kilo which is the body_mass_g divided by 1000. Feel free to manipulate the dataset and create new variables.\n\n# Create a new dataset\nweight = penguins[['body_mass_g']]\n\n#Assign a new variable by common operations\nweight = weight.assign(body_mass_kilo = weight['body_mass_g'] /1000)\nweight.head()\n\n#&gt;    body_mass_g  body_mass_kilo\n#&gt; 0       3750.0            3.75\n#&gt; 1       3800.0            3.80\n#&gt; 2       3250.0            3.25\n#&gt; 4       3450.0            3.45\n#&gt; 5       3650.0            3.65",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "manipulation.html#summarize",
    "href": "manipulation.html#summarize",
    "title": "2  Data manipulation",
    "section": "2.5 Summarize",
    "text": "2.5 Summarize\nHow to calculate summary statistics of the dataset? The summarize function is used to calculate summary statistics of the dataset. The .describe() function is used to calculate summary statistics of the dataset.\nThe summarize function is used to calculate summary statistics of the dataset. The .describe() function is used to calculate summary statistics of the dataset.\n\n#Describe the dataset\npenguins.describe()\n\n#&gt;        bill_length_mm  bill_depth_mm  flipper_length_mm  body_mass_g\n#&gt; count      333.000000     333.000000         333.000000   333.000000\n#&gt; mean        43.992793      17.164865         200.966967  4207.057057\n#&gt; std          5.468668       1.969235          14.015765   805.215802\n#&gt; min         32.100000      13.100000         172.000000  2700.000000\n#&gt; 25%         39.500000      15.600000         190.000000  3550.000000\n#&gt; 50%         44.500000      17.300000         197.000000  4050.000000\n#&gt; 75%         48.600000      18.700000         213.000000  4775.000000\n#&gt; max         59.600000      21.500000         231.000000  6300.000000\n\n\nAnother way to calculate summary statistics is to use the .groupby() function. The .groupby function provides the power of the split-apply-combine pattern. The .groupby function is used to split the dataset into groups. The .apply function is used to apply a function to each group. The .combine function is used to combine the results into a new dataset. For example, calculate the mean body_mass_g of the penguins grouped by sex. Append .groupby() to the dataset and specify the variable you want to group by. In addition, use the .mean() function to calculate the mean of the grouped variable.\n\n# .groupby() and .mean() to calculate the mean body_mass_g of the penguins\npenguins[[\"sex\", \"body_mass_g\"]].groupby(\"sex\").mean()\n\n#&gt;         body_mass_g\n#&gt; sex                \n#&gt; Female  3862.272727\n#&gt; Male    4545.684524\n\n\nThe .groupby function can be used to group by multiple variables. For example, calculate the mean body_mass_g of the penguins grouped by sex and species.\n\n#Add several variables to group by\npenguins.groupby([\"sex\", \"species\"])[\"body_mass_g\"].mean()\n\n#&gt; sex     species  \n#&gt; Female  Adelie       3368.835616\n#&gt;         Chinstrap    3527.205882\n#&gt;         Gentoo       4679.741379\n#&gt; Male    Adelie       4043.493151\n#&gt;         Chinstrap    3938.970588\n#&gt;         Gentoo       5484.836066\n#&gt; Name: body_mass_g, dtype: float64\n\n\nFinally, the .count() function is used to count the number of entries in each category of a variable.\n\n#shortr penguins[\"species\"].value_counts()\npenguins.groupby(\"species\")[\"species\"].count()\n\n#&gt; species\n#&gt; Adelie       146\n#&gt; Chinstrap     68\n#&gt; Gentoo       119\n#&gt; Name: species, dtype: int64\n\n\n\nSummary\nIn this script, we have shown how to use dplyr functions in Python. Keep in mind that the dplyr package is designed for data manipulation in R. However, the pandas library in Python provides similar functions to dplyr. Aggregation statistics can be calculated on entire columns or rows. The groupby function provides the power of the split-apply-combine pattern. The value_counts is a convenient shortcut to count the number of entries in each category of a variable.\nThe .loc function is used to access a group of rows and columns by labels or a boolean array. Say we want to create a indicator variable Adelie. The .loc function is used to create a new variable Adelie. The first line of code creates a new variable Adelie with the .loc function. The second line of code fills the rest of the column with False. The third line of code selects the columns species and Adelie.\n\n# Create a new variable with the .loc function\npenguins.loc[penguins['species'] == \"Adelie\", 'Adelie'] = 'True' \n\n#Fill the rest of the column with False\npenguins.loc[penguins['species'] != \"Adelie\", 'Adelie'] = 'False' \n\n#Select the columns species and Adelie\npenguins[['species', 'Adelie']].head()\n\n#&gt;   species Adelie\n#&gt; 0  Adelie   True\n#&gt; 1  Adelie   True\n#&gt; 2  Adelie   True\n#&gt; 4  Adelie   True\n#&gt; 5  Adelie   True\n\n\nSo say we want to create a new variable check_length which checks if the bill_length_mm is less than or equal to 40. The .apply() function is used to apply a function along an axis of the dataframe. For example, the lambda function is used to create a new variable check_length. It checks if the bill_length_mm is less than or equal to 40. If not, the function returns False. If the condition is met, the function returns True.\n\n# Create a new variable check_length: check if the bill_length_mm is less than or equal to 40\npenguins['check_length'] = penguins['bill_length_mm'].apply(lambda x: 'True' if x &lt;= 40 else 'False')\n\n#Select the columns bill_length_mm and check_length\npenguins[['bill_length_mm', 'check_length']].head()\n\n#&gt;    bill_length_mm check_length\n#&gt; 0            39.1         True\n#&gt; 1            39.5         True\n#&gt; 2            40.3        False\n#&gt; 4            36.7         True\n#&gt; 5            39.3         True",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Data manipulation</span>"
    ]
  },
  {
    "objectID": "visualization.html",
    "href": "visualization.html",
    "title": "3  Visualization",
    "section": "",
    "text": "3.1 Categorical data\nWhen it comes to visualizations, R user know base R plots and very like the ggplot2 package. This chapter shows how to create visualizations with Python packages like plotnine that mimic the logic of the ggplot2 package. The latter implements the grammar of graphics, which is a powerful way to create visualizations. The grammar of graphics is a way to describe the components of a plot, such as the data, the aesthetics, and the geometries, in a structured way. This makes it possible to create complex visualizations by combining components.\nIn this chapter I show how to create common visualizations with Python. First, I show how to create quick visualizations with the matplotlib package, which is the most popular package for creating visualizations in Python. I also show how to create visualizations with the seaborn package, which is another popular package for creating visualizations. The seaborn package is built on top of matplotlib and provides a high-level interface for creating attractive visualizations. Finally, I show how to create visualizations with the plotnine package, which is a Python implementation of the ggplot2 package. If you are familiar with ggplot2, you will feel right at home with plotnine. The plotnine package implements the grammar of graphics in Python.\nLet’s start by loading some data. I will use the penguins dataset from the seaborn package and I import matplotlib. Furthermore, the matplotlib.pyplot module is imported as plt.\nThe latter is a collection of command style functions that make matplotlib work like MATLAB. Each pyplot function makes some change to a figure. For example, a figure can contain multiple plots. The figure is the top-level container for all the plot elements. The axes is the actual plot. The axes contains the x-axis, the y-axis, the title, and the labels. The axes is the container for all the plot elements. The plot elements are the lines, the points, the bars, and the text that make up the plot.\nAs outlined, the searborn package is built on top of matplotlib and provides a high-level interface for creating attractive visualizations. The seaborn package provides a number of functions for creating visualizations of categorical data. For example, you can create bar graphs, box plots, and violin plots with the seaborn package. To create a bar graph of the bill length of the penguins grouped by species, append the .barplot() function to the sns object and specify the data, the x variable, and the y variable.\n#Bar graph\nsns.barplot(data=penguins, x=\"species\", y=\"bill_length_mm\")\nThe catplot() function can be used to create a variety of visualizations of categorical data. For example, you can create a box plot of the bill length of the penguins grouped by species with the catplot() function. To create a box plot, set the kind parameter to “box”.\n#Make a box plot with the catplot() function\nsns.catplot(data=penguins, kind=\"box\", x=\"sex\", y=\"bill_length_mm\")\nBox plots are a good way to visualize the distribution of a continuous variable within different categories, but they do not show the individual data points. A swarm plot is a good way to visualize the individual data points within different categories. A swarm plot is similar to a scatter plot, but it arranges the data points so that they do not overlap. To create a swarm plot of the bill length of the penguins grouped by species, set the kind parameter to “swarm”.\n#Make a swarm plot\nsns.catplot(data=penguins, kind=\"swarm\", x=\"sex\", y=\"bill_length_mm\", hue=\"species\")",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#relationsships",
    "href": "visualization.html#relationsships",
    "title": "3  Visualization",
    "section": "3.2 Relationsships",
    "text": "3.2 Relationsships\nNext, we will focus on histograms and scatter plots. These are common visualizations for exploring numerical variables and the relationship between two continuous variables. To create a histogram of the body mass of the penguins, use the hist() function from the matplotlib package. The hist() function takes the data as input and creates a histogram of the data. The number of bins can be specified with the bins parameter.\n\n#Hist with matplotlib\nplt.hist(penguins.body_mass_g, bins=30)\n\n\n\n\n\n\n\n\nAnd in case of a scatter plot, you can use the scatter() function from the matplotlib package. The scatter() function takes the x-axis variable and the y-axis variable as input and creates a scatter plot of the data. For example, how is the body mass of the penguins related to the bill length?\n\n#Scatterplot with matplotlib\nplt.scatter(penguins.body_mass_g, penguins.bill_length_mm)\n\n\n\n\n\n\n\n\nComing from the R universe, I don’t like the default style of matplotlib. I prefer the ggplot style. To use the ggplot style, you can use the matplotlib.style.use() function and set the style to “ggplot”.\n\n#Adjust the style\nmatplotlib.style.use('ggplot')\nplt.scatter(penguins.body_mass_g, penguins.bill_length_mm)\n\n\n\n\n\n\n\n\nNobody said that the ggplot style is the best. It’s just a matter of taste and I also do not like some details of default ggplot2 style. The good news is, we can adjust all details if we switch back to the ggplot2 approach and the plotnine package. Having an R background, the code to create the same scatter plot with the plotnine package looks very familiar. The ggplot() function creates a plot object; the aes() function is used to specify the x-axis variable and the y-axis variable. The geom_point() function adds points to the plot or more precisely, it adds a geom to the plot. The stat_smooth() function is used to add a linear regression line to the plot. The facet_wrap() function is used to create separate plots for each species.\n\n#Scatterplot with plotnine\nfrom plotnine import *\n\n#Create a scatter plot with plotnine\np = ggplot(penguins, aes('body_mass_g', 'bill_length_mm')) + geom_point()\n\n#Add a linear regression line and facet the plot by species\np = p + stat_smooth(method='lm') + facet_wrap('~species')\n \nprint(p)\n\n\n\n\n\n\n\n\nLook at the next code: We can create a box plot of the body mass of the penguins grouped by species with the geom_boxplot() function. The factor() function is used to convert the species variable to a factor variable. In addition, themes in plotnine can be adjusted like in ggplot2. The theme_dark() function creates a dark theme for the plot or the theme_minimal() function creates a minimal theme for the plot.\n\n# A boxplot with factor variables and a theme\np = ggplot(penguins) + geom_boxplot(aes(x='factor(species)', y='body_mass_g')) + theme_minimal()\n\nprint(p)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "visualization.html#fonts",
    "href": "visualization.html#fonts",
    "title": "3  Visualization",
    "section": "3.3 Fonts",
    "text": "3.3 Fonts\nTo use a custom font in a plot, you need to download the font file and specify the path to the font file. You can download a font file from a website like Google Fonts. For example, to download the Roboto font from Google Fonts, use the following code.\n\nimport urllib.request\n\nurl = 'https://fonts.googleapis.com/css2?family=Roboto'  \n# Replace with the Google Fonts URL\nfont_path = 'Roboto-Regular.ttf'\n\nurllib.request.urlretrieve(url, font_path)\n\nNext, you need to specify the path to the font file and set the font family in the plot. The following code shows how to use the Roboto font in a plot.\n\nimport matplotlib.font_manager as fm\n\nfont_path = 'Roboto-Regular.ttf'  # Path to your downloaded font\nfont_prop = fm.FontProperties(fname=font_path)\nplt.rcParams['font.family'] = font_prop.get_name()\n\nplot = (\n    ggplot(data, aes('x', 'y')) +\n    geom_point() +\n    theme(\n        plot_title=element_text(family=font_prop.get_name(), size=14, weight='bold')\n    )\n)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Visualization</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Ushey, Kevin, JJ Allaire, and Yuan Tang. 2023. “Reticulate:\nInterface to ’Python’.” https://CRAN.R-project.org/package=reticulate.\n\n\nWaskom, Michael L. 2021. “Seaborn: Statistical Data\nVisualization.” Journal of Open Source Software 6 (60):\n3021. https://doi.org/10.21105/joss.03021.",
    "crumbs": [
      "References"
    ]
  }
]